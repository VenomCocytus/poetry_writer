{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kocnBbCS0g7K"
      },
      "source": [
        "## Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jYxkHVO0uEI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout, Activation\n",
        "from keras.layers import LSTM\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import pickle\n",
        "import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0ggVctf0w4c"
      },
      "source": [
        "## Load the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwpfI8O307D-"
      },
      "source": [
        "The next step is to load the data into Python to make it easy to read and access. \n",
        "\n",
        "Here, we will use a combined collection of all Shakespearean sonnets as a dataset for this tutorial but we can choose any book/corpus we want. This collection that can be downloaded from [here](https://www.gutenberg.org/cache/epub/1041/pg1041.txt). I cleaned up this file to remove the start and end credits.\n",
        "\n",
        "The text file is opened and saved in text. This content is then converted into lowercase, to reduce the number of possible words by reducing our **vocabulary** (a list of unique word) and punctuation as well as replacing two sonsecutives newlines with jsut one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk6PypLS8REt"
      },
      "source": [
        "\n",
        "#### Download and Save the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMEosRsb1k_m",
        "outputId": "0c29f1b1-e001-4359-f052-e0e28088c05f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "122408"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a data folder if it doesn't already exist\n",
        "if not os.path.exists(\"data\"):\n",
        "    os.mkdir('data')\n",
        "\n",
        "# Download\n",
        "content = requests.get(\"https://www.gutenberg.org/cache/epub/1041/pg1041.txt\").text\n",
        "\n",
        "# Save\n",
        "open(\"data/sonnets.txt\", \"w\", encoding=\"utf-8\").write(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofqtnvdY5OEG"
      },
      "source": [
        "#### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLiCG6LX5kud"
      },
      "outputs": [],
      "source": [
        "# Data file path\n",
        "file_path = \"data/sonnets.txt\"\n",
        "basename = os.path.basename(file_path)\n",
        "\n",
        "text = open(file_path, encoding=\"utf-8\").read()\n",
        "text = text.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXIJ7RzQ1ln5"
      },
      "source": [
        "## Creating Character/ Word Mappings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "domCDYqR1vsG"
      },
      "source": [
        "Now that we loaded and cleaned the dataset successfully, we need a way to convert these characters into integers, there are a lot of Keras and Scikit-Learn utilities out there for that, but we are going to make this manually in Python.\n",
        "\n",
        "Since we have alphabet as our list  that contains all the unique characters of our dataset, we can make two dictionaries that map each character to an integer number and vice-versa.\n",
        "\n",
        "Then it's time to map it. **Mapping** is a step in which we assign an arbitrary number to a character/word in the text. In this way, all unique characters/words are mapped to a number. This is important, because machines understand numbers far better than text, and this subsequently makes the training process easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoaCrOlg19Ko"
      },
      "outputs": [],
      "source": [
        "characters = sorted(list(set(text)))\n",
        "n_to_char = {n:char for n, char in enumerate(characters)}\n",
        "char_to_n = {char:n for n, char in enumerate(characters)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juHWvfCF2GWk"
      },
      "source": [
        "I have created a dictionary with a number assigned to each unique character present in the text. All unique characters are first stored in characters and are then enumerated.\n",
        "\n",
        "It must also be noted here that I have used character level mappings and not word mappings. However, when compared with each other, a word-based model shows much higher accuracy as compared to a character-based model. This is because the latter model requires a much larger network to learn long-term dependencies as it not only has to remember the sequences of words, but also has to learn to predict a grammatically correct word. However, in case of a word-based model, the latter has already been taken care of.\n",
        "\n",
        "But since this is a small dataset (with 119405 characters), and the number of unique words (4,605 in number) constitute around one-fourth of the data, it would not be a wise decision to train on such a mapping. This is because if we assume that all unique words occurred equally in number (which is not true), we would have a word occurring roughly four times in the entire training dataset, which is just not sufficient to build a text generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwb47GtBceNb"
      },
      "source": [
        "Print some statistics about the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r3z4ZgtciGI",
        "outputId": "6eee80c1-515d-43d5-edd6-b4cfaf0d3413"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique characters:  ['\\n', ' ', '!', '\"', '#', '$', '%', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '\\ufeff']\n",
            "Numbers of unique characters:  60\n",
            "Number of characteres 119405\n"
          ]
        }
      ],
      "source": [
        "n_characters = len(text)\n",
        "n_unique_characters = len(characters)\n",
        "\n",
        "print(\"Unique characters: \", characters)\n",
        "print(\"Numbers of unique characters: \", n_unique_characters)\n",
        "print(\"Number of characteres\", n_characters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olT94kUjfZs6"
      },
      "source": [
        "#### Save dictionnaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2yVB1pofew4"
      },
      "source": [
        "Let's save these two dictionnaries to a file using **pickle** to retrieve them later, when we test our generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqHfExOPgJJl"
      },
      "outputs": [],
      "source": [
        "# save these dictionnary for a later use\n",
        "basename = \"venom\"\n",
        "pickle.dump(char_to_n, open(f\"{basename}-char_to_n.pickle\", \"wb\"))\n",
        "pickle.dump(n_to_char, open(f\"{basename}-n_to_char.pickle\", \"wb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLWujFZI2ooX"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcFTa-tb3SBW"
      },
      "source": [
        "Since text is unstructured data, it has a lot of meaningless words. In text analysis, data must be cleared, processed, and converted into structured data.\n",
        "\n",
        "This is the most tricky part when it comes to building LSTM models. Transforming the data at hand into a relatable format is a difficult task.\n",
        "\n",
        "I’ll break down the process into small pieces to make it easier to understand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2lKkAf2ySyy"
      },
      "outputs": [],
      "source": [
        "sequence_length = 100\n",
        "batch_size = 128\n",
        "epochs = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPsPOfs-lo9T"
      },
      "source": [
        "#### Encode the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYhWcoz3lqw8"
      },
      "source": [
        "We gonna convert each character of the text into it's corresponding integer value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSPR81uYl5lT"
      },
      "outputs": [],
      "source": [
        "# Convert all text into integers\n",
        "encoded_text = np.array([char_to_n[c] for c in text])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wKcsn3bm9nC"
      },
      "source": [
        "#### Create a custom dataset object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sevdcREMmW54"
      },
      "source": [
        "Since we want to scale our code for larger datasets, we need to use **tf.data** API for efficient dataset handling, as a result, create a **tf.data.Dataset** object of this encoded_text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGk-jKUim1tB"
      },
      "outputs": [],
      "source": [
        "# Create a custom dataset object\n",
        "char_ds_object = tf.data.Dataset.from_tensor_slices(encoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL_32_MnnZZP"
      },
      "source": [
        "Now, our new object has all the characters of our text, let's print the first ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcQlgGLorv3N",
        "outputId": "4eeca4b0-d114-4144-9a93-7826f0972ea1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "59 ﻿\n",
            "49 t\n",
            "37 h\n",
            "34 e\n",
            "1  \n",
            "45 p\n",
            "47 r\n",
            "44 o\n",
            "39 j\n",
            "34 e\n",
            "32 c\n",
            "49 t\n",
            "1  \n",
            "36 g\n",
            "50 u\n",
            "49 t\n",
            "34 e\n",
            "43 n\n",
            "31 b\n",
            "34 e\n"
          ]
        }
      ],
      "source": [
        "# Printing some data\n",
        "for characters in char_ds_object.take(20):\n",
        "    print(characters.numpy(), n_to_char[characters.numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMrU6dnDt1YG"
      },
      "source": [
        "#### Construct input sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkOkDtdGtpqI"
      },
      "source": [
        "At this point, we need to build our sequences, as mentionned earlier. We want each input sequence to be a sequence of characters of a specific lenght, store in `sequence_length` variable and the output of a single character that is the next one. For this, luckily, we can use **tf.data.Dataset**'s `batch()` method to gather characters together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNPHlZC0xfhD"
      },
      "outputs": [],
      "source": [
        "# Build Sequences by batching\n",
        "sequences = char_ds_object.batch( 2*sequence_length + 1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPm37tvA5JAi"
      },
      "source": [
        "You notice i've converted the integer sequences into normal text using n_to_char dictionnary built earlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAxwhsqWyWMt",
        "outputId": "4bcf70c3-a943-4ec0-9819-50de18fb05b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the project gutenberg ebook of the sonnets, by william shakespeare\n",
            "\n",
            "this ebook is for the use of anyone anywhere in the united states and\n",
            "most other parts of the world at no cost and with almost no re\n",
            "strictions\n",
            "whatsoever. you may copy it, give it away or re-use it under the terms\n",
            "of the project gutenberg license included with this ebook or online at\n",
            "www.gutenberg.org. if you are not located in the\n",
            " united states, you\n",
            "will have to check the laws of the country where you are located before\n",
            "using this ebook.\n",
            "\n",
            "title: the sonnets\n",
            "\n",
            "author: william shakespeare\n",
            "\n",
            "release date: september, 1997 [ebook #104\n"
          ]
        }
      ],
      "source": [
        "# Print some sequences\n",
        "for sequence in sequences.take(3):\n",
        "    print(''.join([n_to_char[i] for i in sequence.numpy()]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvy4ZpbO5lVx"
      },
      "source": [
        "#### Sample_splitter function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qWMGzUW5qWu"
      },
      "source": [
        "Now, each sample is represented, let's prepare our **train** and **targets**. We have to convert a single sample (sequence of characters) into multiple(train, target) samples. For this task, we can use the `flat_map()` method which takes callback function that loops over all our data samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiMYMwBD3S-y"
      },
      "outputs": [],
      "source": [
        "def sample_splitter(sample):\n",
        "\n",
        "    length = len(sample)\n",
        "    ds = tf.data.Dataset.from_tensors((sample[:sequence_length], sample[sequence_length]))\n",
        "\n",
        "    for i in range(0, length-sequence_length, 1):\n",
        "\n",
        "        sequence = sample[i: i + sequence_length]\n",
        "        label = sample[i + sequence_length]\n",
        "\n",
        "        # extend the dataset with these samples with the concatenate() method\n",
        "        other_ds = tf.data.Dataset.from_tensors((sequence, label))\n",
        "        ds = ds.concatenate(other_ds)\n",
        "    return ds\n",
        "\n",
        "# Prepare sequences and labels\n",
        "dataset = sequences.flat_map(sample_splitter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QE-57cI4NSW"
      },
      "source": [
        "Here, **sequence** is our train array, and **label** is our target array.\n",
        "\n",
        "`sequence_length` is the length of the sequence of characters that we want to consider before predicting a particular character.\n",
        "\n",
        "The for loop is used to iterate over the entire length of the text and create such sequences (stored in `sequence`) and their true values (stored in `label`). Now, it’s difficult to visualize the concept of true values here. Let’s  have a good understanding of the code above with this example:\n",
        "\n",
        "Let's say we have a sequence length of 4 (too small but good for explanation) and the text “hello cameroon”, we would have our `sequence` and `label` (not encoded as numbers for ease of understanding) as below:\n",
        "\n",
        "X\tY\n",
        "[h, e, l, l]\t[o]\n",
        "[e, l, l, o]\t[ ]\n",
        "[l, l, o,  ]\t[c]\n",
        "[l, o,  , i]\t[a]\n",
        "….\t….\n",
        "\n",
        "We do that on all samples, in the end, we'll see that we increased the number of training samples. And we've used the `concatenate()` to join these samples together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrB_J2NBEQWy"
      },
      "source": [
        "#### One-hot Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9xBkw6JEU25"
      },
      "source": [
        "For **categorical** data where no ordinal relationship exists, the integer encoding is not enough. So, using this encoding and allowing the model to assume a natural ordering between categories may result in poor performance or unexpected results (predictions halfway between categories).\n",
        "\n",
        "In our case, a **one-hot encoding** can be applied to the integer representation. This is where the integer encoded variable is removed and a new binary variable is added for each unique integer value.\n",
        "\n",
        "In the example below, for the **color** variable we've 3 categories and therefore 3 binary variables are needed. A “1” value is placed in the binary variable for the color and “0” values for the other colors.\n",
        "\n",
        "| red | green | blue |\n",
        "| --- | ----- | ---- |\n",
        "| 0   | 1     |   0  |\n",
        "| 0   | 0     |  1   |\n",
        "| 1   | 0     | 0    |\n",
        "\n",
        "As a second example, if 'v' is encoded as 3 and n_unique_characters = 7, the result should be the vector: [0, 0, 1, 0, 0, 0, 0], since 'v' is the third character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIsVH2J0GOVu"
      },
      "outputs": [],
      "source": [
        "# One-hot encode the sequences and the labels\n",
        "def one_hot_encoding(sequence, label):\n",
        "    return tf.one_hot(sequence, n_unique_characters), tf.one_hot(label, n_unique_characters)\n",
        "\n",
        "dataset = dataset.map(one_hot_encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGVH1E8yIDjR"
      },
      "source": [
        "We've used the `map()` method to one-hot encode each sample on our dataset, `tf.one_hot()` method did the job. Let's show some data samples and their corresponding shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAlHLvPDImx1",
        "outputId": "735c732e-d4ef-4346-bb1c-3504d9b1e2a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: ﻿the project gutenberg ebook of the sonnets, by william shakespeare\n",
            "\n",
            "this ebook is for the use of an\n",
            "Target: y\n",
            "Input shape: (100, 60)\n",
            "Target shape: (60,)\n",
            "================================================== \n",
            "\n",
            "Input: ﻿the project gutenberg ebook of the sonnets, by william shakespeare\n",
            "\n",
            "this ebook is for the use of an\n",
            "Target: y\n",
            "Input shape: (100, 60)\n",
            "Target shape: (60,)\n",
            "================================================== \n",
            "\n",
            "Input: the project gutenberg ebook of the sonnets, by william shakespeare\n",
            "\n",
            "this ebook is for the use of any\n",
            "Target: o\n",
            "Input shape: (100, 60)\n",
            "Target shape: (60,)\n",
            "================================================== \n",
            "\n",
            "Input: he project gutenberg ebook of the sonnets, by william shakespeare\n",
            "\n",
            "this ebook is for the use of anyo\n",
            "Target: n\n",
            "Input shape: (100, 60)\n",
            "Target shape: (60,)\n",
            "================================================== \n",
            "\n",
            "Input: e project gutenberg ebook of the sonnets, by william shakespeare\n",
            "\n",
            "this ebook is for the use of anyon\n",
            "Target: e\n",
            "Input shape: (100, 60)\n",
            "Target shape: (60,)\n",
            "================================================== \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for e in dataset.take(5):\n",
        "    print(\"Input:\", ''.join([n_to_char[np.argmax(char_vector)] for char_vector in e[0].numpy()]))\n",
        "    print(\"Target:\", n_to_char[np.argmax(e[1].numpy())])\n",
        "    print(\"Input shape:\", e[0].shape)\n",
        "    print(\"Target shape:\", e[1].shape)\n",
        "    print(\"=\"*50, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmPq0OC8S_WW"
      },
      "source": [
        "Each input element has the shape of (sequence_length, alphabet size). In this case, we have 60 unique characters and 100 is the sequence length. The shape of the output is a **one-hot-encoded** one-dimensional vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdopL09xT5tp"
      },
      "source": [
        "#### Repeat, Shuffle and Batch the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3Am7PhqUBkJ"
      },
      "outputs": [],
      "source": [
        "ds = dataset.repeat().shuffle(1024).batch(batch_size, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnIqb7Es5CoI"
      },
      "source": [
        "## Building Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY4-JGtMVM-m"
      },
      "source": [
        "**Modeling** is the most crucial part in text generation. First, the computer is trained to produce text by being fed both sequence and label. In doing so, it is taught to identify various patterns in natural languages. This means that in the future, it can generate an output of its own if it's fed the sequence.\n",
        "\n",
        "Now let's build our model, it has basically 02 LSTM layers(Long Short-term Memory), a form of model that helps predict sequential data; and an arbitrary number of 700 LSTM units. \n",
        "\n",
        "Try to experiment with different model architectures, you're free to do whatever you want!\n",
        "\n",
        "The output layer is a fully-connected layer with 60 units where each neuron corresponds to a character (probability of the occurrence of each character).\n",
        "\n",
        "We're using **Adam optimizer** here but we can use different optimizer in order to evaluate performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnESabtL5Fri",
        "outputId": "70b9be68-e172-418f-9d00-16d39e01b831"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 100, 700)          2130800   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100, 700)          0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 700)               3922800   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 700)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 60)                42060     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,095,660\n",
            "Trainable params: 6,095,660\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(700, input_shape=(sequence_length, n_unique_characters), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(700))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(n_unique_characters, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOrjgSub5SD4"
      },
      "source": [
        "We are building a sequential model with 02 LSTM layers having **700** units each. The first layer needs to be fed in with the input shape. In order for the next LSTM layer to be able to process the same sequences, we enter the `return_sequences` parameter as True.\n",
        "\n",
        "Also, dropout layers with a 20% dropout have been added to check for over-fitting. The last layer outputs a one-hot encoded vector which gives the character output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvdXm8FiXVFz"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWV43wdLXYF1",
        "outputId": "68c43051-f936-4673-8878-bf5bbc4c2ce0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "932/932 [==============================] - 159s 162ms/step - loss: 2.4474\n",
            "Epoch 2/100\n",
            "932/932 [==============================] - 152s 164ms/step - loss: 1.8572\n",
            "Epoch 3/100\n",
            "932/932 [==============================] - 153s 164ms/step - loss: 1.6098\n",
            "Epoch 4/100\n",
            "932/932 [==============================] - 154s 165ms/step - loss: 1.4121\n",
            "Epoch 5/100\n",
            "932/932 [==============================] - 155s 166ms/step - loss: 1.1980\n",
            "Epoch 6/100\n",
            "932/932 [==============================] - 155s 166ms/step - loss: 0.9716\n",
            "Epoch 7/100\n",
            "932/932 [==============================] - 156s 167ms/step - loss: 0.7527\n",
            "Epoch 8/100\n",
            "932/932 [==============================] - 153s 164ms/step - loss: 0.5589\n",
            "Epoch 9/100\n",
            "932/932 [==============================] - 152s 163ms/step - loss: 0.4216\n",
            "Epoch 10/100\n",
            "932/932 [==============================] - 152s 164ms/step - loss: 0.3195\n",
            "Epoch 11/100\n",
            "932/932 [==============================] - 153s 164ms/step - loss: 0.2512\n",
            "Epoch 12/100\n",
            "932/932 [==============================] - 155s 166ms/step - loss: 0.2177\n",
            "Epoch 13/100\n",
            "932/932 [==============================] - 153s 164ms/step - loss: 0.3225\n",
            "Epoch 14/100\n",
            "932/932 [==============================] - 154s 165ms/step - loss: 0.1686\n",
            "Epoch 15/100\n",
            "932/932 [==============================] - 153s 165ms/step - loss: 0.1510\n",
            "Epoch 16/100\n",
            "932/932 [==============================] - 152s 163ms/step - loss: 0.1495\n",
            "Epoch 17/100\n",
            "932/932 [==============================] - 152s 163ms/step - loss: 0.1519\n",
            "Epoch 18/100\n",
            "932/932 [==============================] - 154s 165ms/step - loss: 0.1423\n",
            "Epoch 19/100\n",
            "932/932 [==============================] - 155s 166ms/step - loss: 0.1361\n",
            "Epoch 20/100\n",
            "932/932 [==============================] - 152s 164ms/step - loss: 0.1481\n",
            "Epoch 21/100\n",
            "932/932 [==============================] - 154s 165ms/step - loss: 0.1451\n",
            "Epoch 22/100\n",
            "932/932 [==============================] - 153s 165ms/step - loss: 0.1301\n",
            "Epoch 23/100\n",
            "932/932 [==============================] - 152s 163ms/step - loss: 0.1060\n",
            "Epoch 24/100\n",
            "932/932 [==============================] - 152s 164ms/step - loss: 0.1182\n",
            "Epoch 25/100\n",
            "932/932 [==============================] - 153s 164ms/step - loss: 0.1070\n",
            "Epoch 26/100\n",
            "932/932 [==============================] - 155s 166ms/step - loss: 0.1062\n",
            "Epoch 27/100\n",
            "932/932 [==============================] - 152s 164ms/step - loss: 0.1056\n",
            "Epoch 28/100\n",
            "932/932 [==============================] - 154s 165ms/step - loss: 0.1137\n",
            "Epoch 29/100\n",
            "932/932 [==============================] - 153s 164ms/step - loss: 0.0919\n",
            "Epoch 30/100\n",
            "932/932 [==============================] - 152s 163ms/step - loss: 0.0942\n",
            "Epoch 31/100\n",
            "932/932 [==============================] - 152s 164ms/step - loss: 0.0915\n",
            "Epoch 32/100\n",
            "932/932 [==============================] - 153s 164ms/step - loss: 0.0941\n",
            "Epoch 33/100\n",
            "932/932 [==============================] - 155s 166ms/step - loss: 0.0921\n",
            "Epoch 34/100\n",
            "932/932 [==============================] - 152s 163ms/step - loss: 0.0827\n",
            "Epoch 35/100\n",
            "932/932 [==============================] - 154s 165ms/step - loss: 0.0844\n",
            "Epoch 36/100\n",
            "932/932 [==============================] - 153s 164ms/step - loss: 0.2723\n",
            "Epoch 37/100\n",
            "932/932 [==============================] - 152s 163ms/step - loss: 0.0938\n",
            "Epoch 38/100\n",
            "932/932 [==============================] - 152s 164ms/step - loss: 0.0635\n",
            "Epoch 39/100\n",
            "932/932 [==============================] - 153s 164ms/step - loss: 0.0514\n",
            "Epoch 40/100\n",
            "932/932 [==============================] - 155s 166ms/step - loss: 0.1936\n",
            "Epoch 41/100\n",
            "932/932 [==============================] - 152s 163ms/step - loss: 0.0830\n",
            "Epoch 42/100\n",
            "193/932 [=====>........................] - ETA: 2:01 - loss: 0.0650"
          ]
        }
      ],
      "source": [
        "# define the model path\n",
        "model_weights_path = f\"results/{basename}-{sequence_length}.h5\"\n",
        "\n",
        "# Make a result folder if it doesn't exist or select an existing one\n",
        "if not os.path.isdir('results'):\n",
        "    os.mkdir('results')\n",
        "\n",
        "# Train the model\n",
        "model.fit(ds, steps_per_epoch=(len(encoded_text) - sequence_length) // batch_size, epochs = epochs)\n",
        "\n",
        "# Save the model\n",
        "model.save(model_weights_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybo_zZQpYKwD"
      },
      "source": [
        "We fed the dataset object that we create earlier, and since the model object has no idea on many samples are there in the dataset, we specified `steps_per_epoch` parameter, which is set to the number of training samples divided by the `batch_size`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxDtWQIE5Zc_"
      },
      "source": [
        "## Generate new Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U2SYHdneEzM"
      },
      "source": [
        "Finally, here is the fun part, now that the model is built and trained, we just have to generate our poetry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkXillEYbN5E"
      },
      "source": [
        "#### Create a seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb3gkizCZhke"
      },
      "source": [
        "We need a sample text, a seed to start generating. This will depend on your problem, you can take sentences from the training data in which it will perform better, but we will try to produce a **new chapter** of this book:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CCrgp5cbFlm"
      },
      "outputs": [],
      "source": [
        "seed = 'chapter xviii'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JuuzpFseoIc"
      },
      "source": [
        "if it's a single notebook, You do not have to follow the following three sections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsJxi1OObQip"
      },
      "source": [
        "#### Load dictionnaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO3Mv2mrbaGY"
      },
      "source": [
        "Let's load the dictionnaries that map each integer to a character and vise-versa that we saved before in the **character mappings phase**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoK5_A4zbUcD"
      },
      "outputs": [],
      "source": [
        "# load characters dictionaries\n",
        "char_to_n = pickle.load(open(f\"{basename}-char_to_n.pickle\", \"rb\"))\n",
        "n_to_char = pickle.load(open(f\"{basename}-n_to_char.pickle\", \"rb\"))\n",
        "dict_size = len(char_to_n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8et7nWEbfF4u"
      },
      "source": [
        "#### Rebuild the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vifp0YAdb1Q2"
      },
      "outputs": [],
      "source": [
        "#### Rebuild the model\n",
        "model = Sequential([\n",
        "    LSTM(700, input_shape=(sequence_length, dict_size), return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(700),\n",
        "    Dropout(0.2),\n",
        "    Dense(dict_size, activation='softmax'),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq45mwGVgfC8"
      },
      "source": [
        "#### Load saved weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz0nIibJfv3G"
      },
      "source": [
        "Equally, we need to load the optimal set of model weights. to avoid to retrain the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAuWB9Pef6x2"
      },
      "outputs": [],
      "source": [
        "# load the optimal weights\n",
        "model.load_weights(f\"results/{basename}-{sequence_length}.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGHn30Y1goTR"
      },
      "source": [
        "#### Generate our Poetry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IsJ0q-OhdS2"
      },
      "outputs": [],
      "source": [
        "n_chars = 500\n",
        "\n",
        "# Generating characters\n",
        "generated = \"\"\n",
        "\n",
        "for i in tqdm.tqdm(range(n_chars), \"Generating text\\n\"):\n",
        "\n",
        "    # Make an input sequence\n",
        "    X = np.zeros((1, sequence_length, dict_size))\n",
        "    for t, char in enumerate(seed):\n",
        "        X[0, (sequence_length - len(seed)) + t, char_to_n[char]] = 1\n",
        "    # predict the next character\n",
        "    prediction = model.predict(X, verbose=0)[0]\n",
        "\n",
        "    # converting the vector to an integer\n",
        "    next_index = np.argmax(prediction)\n",
        "\n",
        "    # converting the integer to a character\n",
        "    next_char = n_to_char[next_index]\n",
        "\n",
        "    # add the character to results\n",
        "    generated += next_char\n",
        "\n",
        "    # shift seed and the predicted character\n",
        "    seed = seed[1:] + next_char\n",
        "\n",
        "print(\"Seed:\", seed)\n",
        "print(\"Generated text:\")\n",
        "print(generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0RN17l36SEU"
      },
      "source": [
        "All we've done here is starting with a seed text, naking the input sequence, and predicting the next character on one hand. On the other hand, we shift the input sequence by removing the first character and adding the predicted character. This gives us a slightly changed sequence that still has the expected sequence length.\n",
        "\n",
        "We then feed this updated input sequence to the model to predict another character. Repeating this process `n_chars` times will generate a text with **N** characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxx3vMH0lZOd"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNk7TV7RoG10"
      },
      "source": [
        "It is clearly English that we are reading. However, the sentences don't make much sense. In fact, this result has several causes, in particular the length of our dataset which did not have enough samples. Also, the architecture of our model not being optimal, we find ourselves quite easily in loops repeating words ad infinitum. However, we quickly overcome this concern by adding layers to our sequential model.\n",
        "\n",
        "In our case, after several attempts, we were able to observe **acceptable** parameters for our model. We almost have the impression that our model is really trying to **understand and write** poetry. It's funny.\n",
        "\n",
        "It should be noted that this tutorial does not only apply to text in English but to all languages. Indeed, we could even generate code if we have enough lines of code."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
