{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import dependencies","metadata":{"id":"kocnBbCS0g7K"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout, Activation\nfrom keras.layers import LSTM\n\nimport requests\nimport os\nimport pickle\nimport tqdm\n","metadata":{"id":"7jYxkHVO0uEI","execution":{"iopub.status.busy":"2022-11-28T19:05:12.599277Z","iopub.execute_input":"2022-11-28T19:05:12.599874Z","iopub.status.idle":"2022-11-28T19:05:12.605990Z","shell.execute_reply.started":"2022-11-28T19:05:12.599834Z","shell.execute_reply":"2022-11-28T19:05:12.604955Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Load the Data","metadata":{"id":"F0ggVctf0w4c"}},{"cell_type":"markdown","source":"The next step is to load the data into Python to make it easy to read and access. \n\nHere, we will use a combined collection of all Shakespearean sonnets as a dataset for this tutorial but we can choose any book/corpus we want. This collection that can be downloaded from [here](https://www.gutenberg.org/cache/epub/1041/pg1041.txt). I cleaned up this file to remove the start and end credits.\n\nThe text file is opened and saved in text. This content is then converted into lowercase, to reduce the number of possible words by reducing our **vocabulary** (a list of unique word) and punctuation as well as replacing two consecutives newlines with just one","metadata":{"id":"UwpfI8O307D-"}},{"cell_type":"markdown","source":"\n#### Download and Save the dataset","metadata":{"id":"Pk6PypLS8REt"}},{"cell_type":"code","source":"# Create a data folder if it doesn't already exist\nif not os.path.exists(\"data\"):\n    os.mkdir('data')\n\n# Download\ncontent = requests.get(\"https://www.gutenberg.org/cache/epub/1041/pg1041.txt\").text\n\n# Save\nopen(\"data/sonnets.txt\", \"w\", encoding=\"utf-8\").write(content)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hMEosRsb1k_m","outputId":"0c29f1b1-e001-4359-f052-e0e28088c05f","execution":{"iopub.status.busy":"2022-11-28T19:05:12.619085Z","iopub.execute_input":"2022-11-28T19:05:12.619552Z","iopub.status.idle":"2022-11-28T19:05:19.342045Z","shell.execute_reply.started":"2022-11-28T19:05:12.619526Z","shell.execute_reply":"2022-11-28T19:05:19.340306Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"122408"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Load the data","metadata":{"id":"ofqtnvdY5OEG"}},{"cell_type":"code","source":"# Data file path\nfile_path = \"data/sonnets.txt\"\nbasename = os.path.basename(file_path)\n\ntext = open(file_path, encoding=\"utf-8\").read()\ntext = text.lower()","metadata":{"id":"zLiCG6LX5kud","execution":{"iopub.status.busy":"2022-11-28T19:05:19.343590Z","iopub.execute_input":"2022-11-28T19:05:19.343877Z","iopub.status.idle":"2022-11-28T19:05:19.351184Z","shell.execute_reply.started":"2022-11-28T19:05:19.343852Z","shell.execute_reply":"2022-11-28T19:05:19.350049Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Creating Character/ Word Mappings","metadata":{"id":"tXIJ7RzQ1ln5"}},{"cell_type":"markdown","source":"Now that we loaded and cleaned the dataset successfully, we need a way to convert these characters into integers, there are a lot of Keras and Scikit-Learn utilities out there for that, but we are going to make this manually in Python.\n\nSince we have alphabet as our list  that contains all the unique characters of our dataset, we can make two dictionaries that map each character to an integer number and vice-versa.\n\nThen it's time to map it. **Mapping** is a step in which we assign an arbitrary number to a character/word in the text. In this way, all unique characters/words are mapped to a number. This is important, because machines understand numbers far better than text, and this subsequently makes the training process easier.","metadata":{"id":"domCDYqR1vsG"}},{"cell_type":"code","source":"characters = sorted(list(set(text)))\nn_to_char = {n:char for n, char in enumerate(characters)}\nchar_to_n = {char:n for n, char in enumerate(characters)}","metadata":{"id":"SoaCrOlg19Ko","execution":{"iopub.status.busy":"2022-11-28T19:05:19.352942Z","iopub.execute_input":"2022-11-28T19:05:19.353358Z","iopub.status.idle":"2022-11-28T19:05:19.361911Z","shell.execute_reply.started":"2022-11-28T19:05:19.353301Z","shell.execute_reply":"2022-11-28T19:05:19.360648Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"I have created a dictionary with a number assigned to each unique character present in the text. All unique characters are first stored in characters and are then enumerated.\n\nIt must also be noted here that I have used character level mappings and not word mappings. However, when compared with each other, a word-based model shows much higher accuracy as compared to a character-based model. This is because the latter model requires a much larger network to learn long-term dependencies as it not only has to remember the sequences of words, but also has to learn to predict a grammatically correct word. However, in case of a word-based model, the latter has already been taken care of.\n\nBut since this is a small dataset (with 119405 characters), and the number of unique words (4,605 in number) constitute around one-fourth of the data, it would not be a wise decision to train on such a mapping. This is because if we assume that all unique words occurred equally in number (which is not true), we would have a word occurring roughly four times in the entire training dataset, which is just not sufficient to build a text generator.","metadata":{"id":"juHWvfCF2GWk"}},{"cell_type":"markdown","source":"Print some statistics about the data","metadata":{"id":"Kwb47GtBceNb"}},{"cell_type":"code","source":"n_characters = len(text)\nn_unique_characters = len(characters)\n\nprint(\"Unique characters: \", characters)\nprint(\"Numbers of unique characters: \", n_unique_characters)\nprint(\"Number of characteres\", n_characters)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1r3z4ZgtciGI","outputId":"6eee80c1-515d-43d5-edd6-b4cfaf0d3413","execution":{"iopub.status.busy":"2022-11-28T19:05:19.365947Z","iopub.execute_input":"2022-11-28T19:05:19.366827Z","iopub.status.idle":"2022-11-28T19:05:19.373415Z","shell.execute_reply.started":"2022-11-28T19:05:19.366794Z","shell.execute_reply":"2022-11-28T19:05:19.372286Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Unique characters:  ['\\n', ' ', '!', '\"', '#', '$', '%', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '\\ufeff']\nNumbers of unique characters:  60\nNumber of characteres 119405\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Save dictionnaries","metadata":{"id":"olT94kUjfZs6"}},{"cell_type":"markdown","source":"Let's save these two dictionnaries to a file using **pickle** to retrieve them later, when we test our generator","metadata":{"id":"U2yVB1pofew4"}},{"cell_type":"code","source":"# save these dictionnary for a later use\nbasename = \"venom\"\npickle.dump(char_to_n, open(f\"{basename}-char_to_n.pickle\", \"wb\"))\npickle.dump(n_to_char, open(f\"{basename}-n_to_char.pickle\", \"wb\"))","metadata":{"id":"BqHfExOPgJJl","execution":{"iopub.status.busy":"2022-11-28T19:05:19.374994Z","iopub.execute_input":"2022-11-28T19:05:19.375767Z","iopub.status.idle":"2022-11-28T19:05:19.382523Z","shell.execute_reply.started":"2022-11-28T19:05:19.375731Z","shell.execute_reply":"2022-11-28T19:05:19.381573Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{"id":"eLWujFZI2ooX"}},{"cell_type":"markdown","source":"Since text is unstructured data, it has a lot of meaningless words. In text analysis, data must be cleared, processed, and converted into structured data.\n\nThis is the most tricky part when it comes to building LSTM models. Transforming the data at hand into a relatable format is a difficult task.\n\nI’ll break down the process into small pieces to make it easier to understand","metadata":{"id":"qcFTa-tb3SBW"}},{"cell_type":"code","source":"sequence_length = 100\nbatch_size = 128\nepochs = 100","metadata":{"id":"x2lKkAf2ySyy","execution":{"iopub.status.busy":"2022-11-28T19:05:19.384043Z","iopub.execute_input":"2022-11-28T19:05:19.384742Z","iopub.status.idle":"2022-11-28T19:05:19.391222Z","shell.execute_reply.started":"2022-11-28T19:05:19.384707Z","shell.execute_reply":"2022-11-28T19:05:19.390291Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### Encode the data","metadata":{"id":"yPsPOfs-lo9T"}},{"cell_type":"markdown","source":"We gonna convert each character of the text into it's corresponding integer value","metadata":{"id":"KYhWcoz3lqw8"}},{"cell_type":"code","source":"# Convert all text into integers\nencoded_text = np.array([char_to_n[c] for c in text])","metadata":{"id":"NSPR81uYl5lT","execution":{"iopub.status.busy":"2022-11-28T19:05:19.392721Z","iopub.execute_input":"2022-11-28T19:05:19.393472Z","iopub.status.idle":"2022-11-28T19:05:19.417280Z","shell.execute_reply.started":"2022-11-28T19:05:19.393437Z","shell.execute_reply":"2022-11-28T19:05:19.416444Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"#### Create a custom dataset object","metadata":{"id":"0wKcsn3bm9nC"}},{"cell_type":"markdown","source":"Since we want to scale our code for larger datasets, we need to use **tf.data** API for efficient dataset handling, as a result, create a **tf.data.Dataset** object of this encoded_text data","metadata":{"id":"sevdcREMmW54"}},{"cell_type":"code","source":"# Create a custom dataset object\nchar_ds_object = tf.data.Dataset.from_tensor_slices(encoded_text)","metadata":{"id":"pGk-jKUim1tB","execution":{"iopub.status.busy":"2022-11-28T19:05:19.420740Z","iopub.execute_input":"2022-11-28T19:05:19.421018Z","iopub.status.idle":"2022-11-28T19:05:19.439589Z","shell.execute_reply.started":"2022-11-28T19:05:19.420994Z","shell.execute_reply":"2022-11-28T19:05:19.438652Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Now, our new object has all the characters of our text, let's print the first ones","metadata":{"id":"iL_32_MnnZZP"}},{"cell_type":"code","source":"# Printing some data\nfor characters in char_ds_object.take(20):\n    print(characters.numpy(), n_to_char[characters.numpy()])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NcQlgGLorv3N","outputId":"4eeca4b0-d114-4144-9a93-7826f0972ea1","execution":{"iopub.status.busy":"2022-11-28T19:05:19.440790Z","iopub.execute_input":"2022-11-28T19:05:19.441331Z","iopub.status.idle":"2022-11-28T19:05:19.482462Z","shell.execute_reply.started":"2022-11-28T19:05:19.441281Z","shell.execute_reply":"2022-11-28T19:05:19.481535Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"59 ﻿\n49 t\n37 h\n34 e\n1  \n45 p\n47 r\n44 o\n39 j\n34 e\n32 c\n49 t\n1  \n36 g\n50 u\n49 t\n34 e\n43 n\n31 b\n34 e\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Construct input sample","metadata":{"id":"XMrU6dnDt1YG"}},{"cell_type":"markdown","source":"At this point, we need to build our sequences, as mentionned earlier. We want each input sequence to be a sequence of characters of a specific lenght, store in `sequence_length` variable and the output of a single character that is the next one. For this, luckily, we can use **tf.data.Dataset**'s `batch()` method to gather characters together","metadata":{"id":"QkOkDtdGtpqI"}},{"cell_type":"code","source":"# Build Sequences by batching\nsequences = char_ds_object.batch( 2*sequence_length + 1, drop_remainder=True)","metadata":{"id":"KNPHlZC0xfhD","execution":{"iopub.status.busy":"2022-11-28T19:05:19.486710Z","iopub.execute_input":"2022-11-28T19:05:19.487408Z","iopub.status.idle":"2022-11-28T19:05:19.493644Z","shell.execute_reply.started":"2022-11-28T19:05:19.487373Z","shell.execute_reply":"2022-11-28T19:05:19.492709Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"You notice i've converted the integer sequences into normal text using n_to_char dictionnary built earlier","metadata":{"id":"CPm37tvA5JAi"}},{"cell_type":"code","source":"# Print some sequences\nfor sequence in sequences.take(3):\n    print(''.join([n_to_char[i] for i in sequence.numpy()]))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uAxwhsqWyWMt","outputId":"4bcf70c3-a943-4ec0-9819-50de18fb05b2","execution":{"iopub.status.busy":"2022-11-28T19:05:19.496539Z","iopub.execute_input":"2022-11-28T19:05:19.496972Z","iopub.status.idle":"2022-11-28T19:05:19.510042Z","shell.execute_reply.started":"2022-11-28T19:05:19.496943Z","shell.execute_reply":"2022-11-28T19:05:19.509016Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"﻿the project gutenberg ebook of the sonnets, by william shakespeare\n\nthis ebook is for the use of anyone anywhere in the united states and\nmost other parts of the world at no cost and with almost no re\nstrictions\nwhatsoever. you may copy it, give it away or re-use it under the terms\nof the project gutenberg license included with this ebook or online at\nwww.gutenberg.org. if you are not located in the\n united states, you\nwill have to check the laws of the country where you are located before\nusing this ebook.\n\ntitle: the sonnets\n\nauthor: william shakespeare\n\nrelease date: september, 1997 [ebook #104\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Sample_splitter function","metadata":{"id":"vvy4ZpbO5lVx"}},{"cell_type":"markdown","source":"Now, each sample is represented, let's prepare our **train** and **targets**. We have to convert a single sample (sequence of characters) into multiple(train, target) samples. For this task, we can use the `flat_map()` method which takes callback function that loops over all our data samples","metadata":{"id":"2qWMGzUW5qWu"}},{"cell_type":"code","source":"def sample_splitter(sample):\n\n    length = len(sample)\n    ds = tf.data.Dataset.from_tensors((sample[:sequence_length], sample[sequence_length]))\n\n    for i in range(0, length-sequence_length, 1):\n\n        sequence = sample[i: i + sequence_length]\n        label = sample[i + sequence_length]\n\n        # extend the dataset with these samples with the concatenate() method\n        other_ds = tf.data.Dataset.from_tensors((sequence, label))\n        ds = ds.concatenate(other_ds)\n    return ds\n\n# Prepare sequences and labels\ndataset = sequences.flat_map(sample_splitter)","metadata":{"id":"tiMYMwBD3S-y","execution":{"iopub.status.busy":"2022-11-28T19:05:19.511626Z","iopub.execute_input":"2022-11-28T19:05:19.512034Z","iopub.status.idle":"2022-11-28T19:05:20.342641Z","shell.execute_reply.started":"2022-11-28T19:05:19.511998Z","shell.execute_reply":"2022-11-28T19:05:20.341645Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Here, **sequence** is our train array, and **label** is our target array.\n\n`sequence_length` is the length of the sequence of characters that we want to consider before predicting a particular character.\n\nThe for loop is used to iterate over the entire length of the text and create such sequences (stored in `sequence`) and their true values (stored in `label`). Now, it’s difficult to visualize the concept of true values here. Let’s  have a good understanding of the code above with this example:\n\nLet's say we have a sequence length of 4 (too small but good for explanation) and the text “hello cameroon”, we would have our `sequence` and `label` (not encoded as numbers for ease of understanding) as below:\n\nX\tY\n[h, e, l, l]\t[o]\n[e, l, l, o]\t[ ]\n[l, l, o,  ]\t[c]\n[l, o,  , i]\t[a]\n….\t….\n\nWe do that on all samples, in the end, we'll see that we increased the number of training samples. And we've used the `concatenate()` to join these samples together.","metadata":{"id":"8QE-57cI4NSW"}},{"cell_type":"markdown","source":"#### One-hot Encoding","metadata":{"id":"KrB_J2NBEQWy"}},{"cell_type":"markdown","source":"For **categorical** data where no ordinal relationship exists, the integer encoding is not enough. So, using this encoding and allowing the model to assume a natural ordering between categories may result in poor performance or unexpected results (predictions halfway between categories).\n\nIn our case, a **one-hot encoding** can be applied to the integer representation. This is where the integer encoded variable is removed and a new binary variable is added for each unique integer value.\n\nIn the example below, for the **color** variable we've 3 categories and therefore 3 binary variables are needed. A “1” value is placed in the binary variable for the color and “0” values for the other colors.\n\n| red | green | blue |\n| --- | ----- | ---- |\n| 0   | 1     |   0  |\n| 0   | 0     |  1   |\n| 1   | 0     | 0    |\n\nAs a second example, if 'v' is encoded as 3 and n_unique_characters = 7, the result should be the vector: [0, 0, 1, 0, 0, 0, 0], since 'v' is the third character","metadata":{"id":"Y9xBkw6JEU25"}},{"cell_type":"code","source":"# One-hot encode the sequences and the labels\ndef one_hot_encoding(sequence, label):\n    return tf.one_hot(sequence, n_unique_characters), tf.one_hot(label, n_unique_characters)\n\ndataset = dataset.map(one_hot_encoding)","metadata":{"id":"iIsVH2J0GOVu","execution":{"iopub.status.busy":"2022-11-28T19:05:20.345854Z","iopub.execute_input":"2022-11-28T19:05:20.346555Z","iopub.status.idle":"2022-11-28T19:05:20.391917Z","shell.execute_reply.started":"2022-11-28T19:05:20.346515Z","shell.execute_reply":"2022-11-28T19:05:20.391049Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"We've used the `map()` method to one-hot encode each sample on our dataset, `tf.one_hot()` method did the job. Let's show some data samples and their corresponding shapes","metadata":{"id":"ZGVH1E8yIDjR"}},{"cell_type":"code","source":"for e in dataset.take(5):\n    print(\"Input:\", ''.join([n_to_char[np.argmax(char_vector)] for char_vector in e[0].numpy()]))\n    print(\"Target:\", n_to_char[np.argmax(e[1].numpy())])\n    print(\"Input shape:\", e[0].shape)\n    print(\"Target shape:\", e[1].shape)\n    print(\"=\"*50, \"\\n\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WAlHLvPDImx1","outputId":"735c732e-d4ef-4346-bb1c-3504d9b1e2a8","execution":{"iopub.status.busy":"2022-11-28T19:05:20.393562Z","iopub.execute_input":"2022-11-28T19:05:20.393920Z","iopub.status.idle":"2022-11-28T19:05:20.931940Z","shell.execute_reply.started":"2022-11-28T19:05:20.393886Z","shell.execute_reply":"2022-11-28T19:05:20.931034Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"2022-11-28 19:05:20.548794: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Input: ﻿the project gutenberg ebook of the sonnets, by william shakespeare\n\nthis ebook is for the use of an\nTarget: y\nInput shape: (100, 60)\nTarget shape: (60,)\n================================================== \n\nInput: ﻿the project gutenberg ebook of the sonnets, by william shakespeare\n\nthis ebook is for the use of an\nTarget: y\nInput shape: (100, 60)\nTarget shape: (60,)\n================================================== \n\nInput: the project gutenberg ebook of the sonnets, by william shakespeare\n\nthis ebook is for the use of any\nTarget: o\nInput shape: (100, 60)\nTarget shape: (60,)\n================================================== \n\nInput: he project gutenberg ebook of the sonnets, by william shakespeare\n\nthis ebook is for the use of anyo\nTarget: n\nInput shape: (100, 60)\nTarget shape: (60,)\n================================================== \n\nInput: e project gutenberg ebook of the sonnets, by william shakespeare\n\nthis ebook is for the use of anyon\nTarget: e\nInput shape: (100, 60)\nTarget shape: (60,)\n================================================== \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Each input element has the shape of (sequence_length, alphabet size). In this case, we have 60 unique characters and 100 is the sequence length. The shape of the output is a **one-hot-encoded** one-dimensional vector","metadata":{"id":"QmPq0OC8S_WW"}},{"cell_type":"markdown","source":"#### Repeat, Shuffle and Batch the dataset","metadata":{"id":"gdopL09xT5tp"}},{"cell_type":"code","source":"ds = dataset.repeat().shuffle(1024).batch(batch_size, drop_remainder=True)","metadata":{"id":"B3Am7PhqUBkJ","execution":{"iopub.status.busy":"2022-11-28T19:05:20.936292Z","iopub.execute_input":"2022-11-28T19:05:20.940613Z","iopub.status.idle":"2022-11-28T19:05:20.954732Z","shell.execute_reply.started":"2022-11-28T19:05:20.940569Z","shell.execute_reply":"2022-11-28T19:05:20.953595Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Building Model","metadata":{"id":"EnIqb7Es5CoI"}},{"cell_type":"markdown","source":"**Modeling** is the most crucial part in text generation. First, the computer is trained to produce text by being fed both sequence and label. In doing so, it is taught to identify various patterns in natural languages. This means that in the future, it can generate an output of its own if it's fed the sequence.\n\nNow let's build our model, it has basically 02 LSTM layers(Long Short-term Memory), a form of model that helps predict sequential data; and an arbitrary number of 700 LSTM units. \n\nTry to experiment with different model architectures, you're free to do whatever you want!\n\nThe output layer is a fully-connected layer with 60 units where each neuron corresponds to a character (probability of the occurrence of each character).\n\nWe're using **Adam optimizer** here but we can use different optimizer in order to evaluate performance.\n\n","metadata":{"id":"FY4-JGtMVM-m"}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(700, input_shape=(sequence_length, n_unique_characters), return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(700))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(n_unique_characters, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\nmodel.summary()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tnESabtL5Fri","outputId":"70b9be68-e172-418f-9d00-16d39e01b831","execution":{"iopub.status.busy":"2022-11-28T19:05:20.958782Z","iopub.execute_input":"2022-11-28T19:05:20.959443Z","iopub.status.idle":"2022-11-28T19:05:22.071467Z","shell.execute_reply.started":"2022-11-28T19:05:20.959410Z","shell.execute_reply":"2022-11-28T19:05:22.070462Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm (LSTM)                  (None, 100, 700)          2130800   \n_________________________________________________________________\ndropout (Dropout)            (None, 100, 700)          0         \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 700)               3922800   \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 700)               0         \n_________________________________________________________________\ndense (Dense)                (None, 60)                42060     \n=================================================================\nTotal params: 6,095,660\nTrainable params: 6,095,660\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We are building a sequential model with 02 LSTM layers having **700** units each. The first layer needs to be fed in with the input shape. In order for the next LSTM layer to be able to process the same sequences, we enter the `return_sequences` parameter as True.\n\nAlso, dropout layers with a 20% dropout have been added to check for over-fitting. The last layer outputs a one-hot encoded vector which gives the character output.","metadata":{"id":"WOrjgSub5SD4"}},{"cell_type":"markdown","source":"## Training the Model","metadata":{"id":"gvdXm8FiXVFz"}},{"cell_type":"code","source":"# define the model path\nmodel_weights_path = f\"results/{basename}-{sequence_length}.h5\"\n\n# Make a result folder if it doesn't exist or select an existing one\nif not os.path.isdir('results'):\n    os.mkdir('results')\n\n# Train the model\nmodel.fit(ds, steps_per_epoch=(len(encoded_text) - sequence_length) // batch_size, epochs = epochs)\n\n# Save the model\nmodel.save(model_weights_path)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hWV43wdLXYF1","outputId":"68c43051-f936-4673-8878-bf5bbc4c2ce0","execution":{"iopub.status.busy":"2022-11-28T19:05:22.073099Z","iopub.execute_input":"2022-11-28T19:05:22.073733Z","iopub.status.idle":"2022-11-28T21:56:02.976262Z","shell.execute_reply.started":"2022-11-28T19:05:22.073696Z","shell.execute_reply":"2022-11-28T21:56:02.975277Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"2022-11-28 19:05:25.556227: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"},{"name":"stdout","text":"932/932 [==============================] - 107s 110ms/step - loss: 2.3602 - accuracy: 0.3313\nEpoch 2/100\n932/932 [==============================] - 102s 110ms/step - loss: 1.7956 - accuracy: 0.4654\nEpoch 3/100\n932/932 [==============================] - 103s 110ms/step - loss: 1.5539 - accuracy: 0.5252\nEpoch 4/100\n932/932 [==============================] - 103s 110ms/step - loss: 1.3388 - accuracy: 0.5819\nEpoch 5/100\n932/932 [==============================] - 103s 110ms/step - loss: 1.0957 - accuracy: 0.6517\nEpoch 6/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.8357 - accuracy: 0.7333\nEpoch 7/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.6063 - accuracy: 0.8076\nEpoch 8/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.4338 - accuracy: 0.8619\nEpoch 9/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.3114 - accuracy: 0.9031\nEpoch 10/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.2467 - accuracy: 0.9232\nEpoch 11/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.2014 - accuracy: 0.9370\nEpoch 12/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.1755 - accuracy: 0.9448\nEpoch 13/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.1928 - accuracy: 0.9375\nEpoch 14/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.1466 - accuracy: 0.9536\nEpoch 15/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.1336 - accuracy: 0.9580\nEpoch 16/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.1322 - accuracy: 0.9576\nEpoch 17/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.1376 - accuracy: 0.9563\nEpoch 18/100\n932/932 [==============================] - 103s 111ms/step - loss: 0.1347 - accuracy: 0.9558\nEpoch 19/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.1283 - accuracy: 0.9582\nEpoch 20/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.1205 - accuracy: 0.9611\nEpoch 21/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.1245 - accuracy: 0.9588\nEpoch 22/100\n932/932 [==============================] - 102s 109ms/step - loss: 0.1183 - accuracy: 0.9619\nEpoch 23/100\n932/932 [==============================] - 101s 109ms/step - loss: 0.1160 - accuracy: 0.9614\nEpoch 24/100\n932/932 [==============================] - 102s 109ms/step - loss: 0.1110 - accuracy: 0.9634\nEpoch 25/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.1153 - accuracy: 0.9622\nEpoch 26/100\n932/932 [==============================] - 102s 109ms/step - loss: 0.1113 - accuracy: 0.9630\nEpoch 27/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.1008 - accuracy: 0.9672\nEpoch 28/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.1075 - accuracy: 0.9645\nEpoch 29/100\n932/932 [==============================] - 102s 109ms/step - loss: 0.0974 - accuracy: 0.9684\nEpoch 30/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0972 - accuracy: 0.9679\nEpoch 31/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0976 - accuracy: 0.9680\nEpoch 32/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0982 - accuracy: 0.9680\nEpoch 33/100\n932/932 [==============================] - 102s 109ms/step - loss: 0.0939 - accuracy: 0.9687\nEpoch 34/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0856 - accuracy: 0.9718\nEpoch 35/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0837 - accuracy: 0.9720\nEpoch 36/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0956 - accuracy: 0.9689\nEpoch 37/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0810 - accuracy: 0.9739\nEpoch 38/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0834 - accuracy: 0.9726\nEpoch 39/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0837 - accuracy: 0.9728\nEpoch 40/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0816 - accuracy: 0.9732\nEpoch 41/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0806 - accuracy: 0.9732\nEpoch 42/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0749 - accuracy: 0.9757\nEpoch 43/100\n932/932 [==============================] - 103s 111ms/step - loss: 0.0737 - accuracy: 0.9756\nEpoch 44/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0743 - accuracy: 0.9759\nEpoch 45/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0773 - accuracy: 0.9747\nEpoch 46/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0677 - accuracy: 0.9776\nEpoch 47/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0698 - accuracy: 0.9771\nEpoch 48/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0694 - accuracy: 0.9776\nEpoch 49/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0703 - accuracy: 0.9766\nEpoch 50/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0649 - accuracy: 0.9785\nEpoch 51/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0704 - accuracy: 0.9767\nEpoch 52/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0629 - accuracy: 0.9797\nEpoch 53/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0660 - accuracy: 0.9779\nEpoch 54/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0610 - accuracy: 0.9802\nEpoch 55/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0608 - accuracy: 0.9802\nEpoch 56/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0591 - accuracy: 0.9807\nEpoch 57/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0558 - accuracy: 0.9812\nEpoch 58/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0569 - accuracy: 0.9819\nEpoch 59/100\n932/932 [==============================] - 102s 109ms/step - loss: 0.0547 - accuracy: 0.9822\nEpoch 60/100\n932/932 [==============================] - 102s 109ms/step - loss: 0.0541 - accuracy: 0.9823\nEpoch 61/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0563 - accuracy: 0.9814\nEpoch 62/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0551 - accuracy: 0.9820\nEpoch 63/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0509 - accuracy: 0.9833\nEpoch 64/100\n932/932 [==============================] - 102s 109ms/step - loss: 0.0543 - accuracy: 0.9821\nEpoch 65/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0551 - accuracy: 0.9817\nEpoch 66/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0525 - accuracy: 0.9830\nEpoch 67/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0497 - accuracy: 0.9843\nEpoch 68/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0458 - accuracy: 0.9859\nEpoch 69/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0459 - accuracy: 0.9854\nEpoch 70/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0507 - accuracy: 0.9832\nEpoch 71/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0509 - accuracy: 0.9835\nEpoch 72/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0479 - accuracy: 0.9845\nEpoch 73/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0471 - accuracy: 0.9849\nEpoch 74/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0469 - accuracy: 0.9847\nEpoch 75/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0439 - accuracy: 0.9855\nEpoch 76/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0464 - accuracy: 0.9847\nEpoch 77/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0453 - accuracy: 0.9852\nEpoch 78/100\n932/932 [==============================] - 102s 109ms/step - loss: 0.0448 - accuracy: 0.9851\nEpoch 79/100\n932/932 [==============================] - 102s 109ms/step - loss: 0.0455 - accuracy: 0.9849\nEpoch 80/100\n932/932 [==============================] - 102s 109ms/step - loss: 0.0423 - accuracy: 0.9860\nEpoch 81/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0432 - accuracy: 0.9861\nEpoch 82/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0433 - accuracy: 0.9858\nEpoch 83/100\n932/932 [==============================] - 103s 110ms/step - loss: 0.0388 - accuracy: 0.9874\nEpoch 84/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0440 - accuracy: 0.9858\nEpoch 85/100\n932/932 [==============================] - 102s 109ms/step - loss: 0.0424 - accuracy: 0.9863\nEpoch 86/100\n932/932 [==============================] - 102s 109ms/step - loss: 0.0381 - accuracy: 0.9879\nEpoch 87/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0395 - accuracy: 0.9873\nEpoch 88/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0381 - accuracy: 0.9876\nEpoch 89/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0366 - accuracy: 0.9886\nEpoch 90/100\n932/932 [==============================] - 102s 109ms/step - loss: 0.0370 - accuracy: 0.9879\nEpoch 91/100\n932/932 [==============================] - 102s 109ms/step - loss: 0.0395 - accuracy: 0.9872\nEpoch 92/100\n932/932 [==============================] - 102s 109ms/step - loss: 0.0367 - accuracy: 0.9882\nEpoch 93/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0366 - accuracy: 0.9881\nEpoch 94/100\n932/932 [==============================] - 102s 109ms/step - loss: 0.0367 - accuracy: 0.9881\nEpoch 95/100\n932/932 [==============================] - 102s 109ms/step - loss: 0.0365 - accuracy: 0.9884\nEpoch 96/100\n932/932 [==============================] - 102s 109ms/step - loss: 0.0354 - accuracy: 0.9886\nEpoch 97/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0375 - accuracy: 0.9881\nEpoch 98/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0377 - accuracy: 0.9875\nEpoch 99/100\n932/932 [==============================] - 102s 110ms/step - loss: 0.0338 - accuracy: 0.9890\nEpoch 100/100\n932/932 [==============================] - 102s 109ms/step - loss: 0.0336 - accuracy: 0.9891\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We fed the dataset object that we create earlier, and since the model object has no idea on many samples are there in the dataset, we specified `steps_per_epoch` parameter, which is set to the number of training samples divided by the `batch_size`.","metadata":{"id":"Ybo_zZQpYKwD"}},{"cell_type":"markdown","source":"## Generate new Text","metadata":{"id":"nxDtWQIE5Zc_"}},{"cell_type":"markdown","source":"Finally, here is the fun part, now that the model is built and trained, we just have to generate our poetry","metadata":{"id":"_U2SYHdneEzM"}},{"cell_type":"markdown","source":"#### Create a seed","metadata":{"id":"XkXillEYbN5E"}},{"cell_type":"markdown","source":"We need a sample text, a seed to start generating. This will depend on your problem, you can take sentences from the training data in which it will perform better, but we will try to produce a **new chapter** of this book:","metadata":{"id":"eb3gkizCZhke"}},{"cell_type":"code","source":"seed = 'chapter xviii'","metadata":{"id":"7CCrgp5cbFlm","execution":{"iopub.status.busy":"2022-11-28T21:56:02.977952Z","iopub.execute_input":"2022-11-28T21:56:02.978363Z","iopub.status.idle":"2022-11-28T21:56:02.983529Z","shell.execute_reply.started":"2022-11-28T21:56:02.978304Z","shell.execute_reply":"2022-11-28T21:56:02.982372Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"if it's a single notebook, You do not have to follow the following three sections","metadata":{"id":"_JuuzpFseoIc"}},{"cell_type":"markdown","source":"#### Load dictionnaries","metadata":{"id":"IsJxi1OObQip"}},{"cell_type":"markdown","source":"Let's load the dictionnaries that map each integer to a character and vise-versa that we saved before in the **character mappings phase**","metadata":{"id":"QO3Mv2mrbaGY"}},{"cell_type":"code","source":"# load characters dictionaries\nchar_to_n = pickle.load(open(f\"{basename}-char_to_n.pickle\", \"rb\"))\nn_to_char = pickle.load(open(f\"{basename}-n_to_char.pickle\", \"rb\"))\ndict_size = len(char_to_n)","metadata":{"id":"GoK5_A4zbUcD","execution":{"iopub.status.busy":"2022-11-28T21:56:02.985017Z","iopub.execute_input":"2022-11-28T21:56:02.985554Z","iopub.status.idle":"2022-11-28T21:56:02.993912Z","shell.execute_reply.started":"2022-11-28T21:56:02.985518Z","shell.execute_reply":"2022-11-28T21:56:02.992866Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"#### Rebuild the model","metadata":{"id":"8et7nWEbfF4u"}},{"cell_type":"code","source":"#### Rebuild the model\nmodel = Sequential([\n    LSTM(700, input_shape=(sequence_length, dict_size), return_sequences=True),\n    Dropout(0.2),\n    LSTM(700),\n    Dropout(0.2),\n    Dense(dict_size, activation='softmax'),\n])","metadata":{"id":"Vifp0YAdb1Q2","execution":{"iopub.status.busy":"2022-11-28T21:56:02.995339Z","iopub.execute_input":"2022-11-28T21:56:02.995899Z","iopub.status.idle":"2022-11-28T21:56:03.970173Z","shell.execute_reply.started":"2022-11-28T21:56:02.995858Z","shell.execute_reply":"2022-11-28T21:56:03.969256Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"#### Load saved weights","metadata":{"id":"bq45mwGVgfC8"}},{"cell_type":"markdown","source":"Equally, we need to load the optimal set of model weights. to avoid to retrain the model","metadata":{"id":"Uz0nIibJfv3G"}},{"cell_type":"code","source":"# load the optimal weights\nmodel.load_weights(f\"results/{basename}-{sequence_length}.h5\")","metadata":{"id":"LAuWB9Pef6x2","execution":{"iopub.status.busy":"2022-11-28T21:56:03.971637Z","iopub.execute_input":"2022-11-28T21:56:03.971974Z","iopub.status.idle":"2022-11-28T21:56:04.007995Z","shell.execute_reply.started":"2022-11-28T21:56:03.971931Z","shell.execute_reply":"2022-11-28T21:56:04.007100Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"#### Generate our Poetry","metadata":{"id":"TGHn30Y1goTR"}},{"cell_type":"code","source":"n_chars = 500\n\n# Generating characters\ngenerated = \"\"\n\nfor i in tqdm.tqdm(range(n_chars), \"Generating text\\n\"):\n\n    # Make an input sequence\n    X = np.zeros((1, sequence_length, dict_size))\n    for t, char in enumerate(seed):\n        X[0, (sequence_length - len(seed)) + t, char_to_n[char]] = 1\n    # predict the next character\n    prediction = model.predict(X, verbose=0)[0]\n\n    # converting the vector to an integer\n    next_index = np.argmax(prediction)\n\n    # converting the integer to a character\n    next_char = n_to_char[next_index]\n\n    # add the character to results\n    generated += next_char\n\n    # shift seed and the predicted character\n    seed = seed[1:] + next_char\n\nprint(\"Seed:\", seed)\nprint(\"Generated text:\")\nprint(generated)","metadata":{"id":"1IsJ0q-OhdS2","execution":{"iopub.status.busy":"2022-11-28T21:56:04.009416Z","iopub.execute_input":"2022-11-28T21:56:04.009750Z","iopub.status.idle":"2022-11-28T21:56:27.284804Z","shell.execute_reply.started":"2022-11-28T21:56:04.009717Z","shell.execute_reply":"2022-11-28T21:56:27.283833Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Generating text\nGenerating text  | 0/500 [00:00<?, ?it/s]\nGenerating text  | 1/500 [00:00<04:38,  1.79it/s]\nGenerating text  | 4/500 [00:00<01:09,  7.13it/s]\nGenerating text  | 7/500 [00:00<00:41, 11.79it/s]\nGenerating text  | 10/500 [00:00<00:31, 15.61it/s]\nGenerating text  | 13/500 [00:01<00:28, 16.89it/s]\nGenerating text  | 16/500 [00:01<00:26, 18.42it/s]\nGenerating text  | 19/500 [00:01<00:25, 19.10it/s]\nGenerating text  | 22/500 [00:01<00:22, 20.78it/s]\nGenerating text  | 25/500 [00:01<00:24, 19.33it/s]\nGenerating text  | 28/500 [00:01<00:25, 18.65it/s]\nGenerating text  | 30/500 [00:01<00:26, 17.62it/s]\nGenerating text  | 32/500 [00:02<00:27, 17.12it/s]\nGenerating text  | 34/500 [00:02<00:27, 16.85it/s]\nGenerating text  | 36/500 [00:02<00:27, 16.69it/s]\nGenerating text  | 38/500 [00:02<00:27, 16.66it/s]\nGenerating text  | 40/500 [00:02<00:27, 16.86it/s]\nGenerating text  | 43/500 [00:02<00:24, 18.50it/s]\nGenerating text  | 46/500 [00:02<00:22, 20.04it/s]\nGenerating text  | 49/500 [00:02<00:21, 20.74it/s]\nGenerating text  | 52/500 [00:03<00:20, 22.28it/s]\nGenerating text  | 55/500 [00:03<00:19, 23.41it/s]\nGenerating text  | 58/500 [00:03<00:19, 22.69it/s]\nGenerating text  | 61/500 [00:03<00:18, 23.13it/s]\nGenerating text  | 64/500 [00:03<00:18, 23.51it/s]\nGenerating text  | 67/500 [00:03<00:18, 23.41it/s]\nGenerating text  | 70/500 [00:03<00:18, 23.60it/s]\nGenerating text  | 73/500 [00:03<00:19, 21.51it/s]\nGenerating text  | 76/500 [00:04<00:19, 22.21it/s]\nGenerating text  | 79/500 [00:04<00:20, 20.99it/s]\nGenerating text  | 82/500 [00:04<00:19, 21.37it/s]\nGenerating text  | 85/500 [00:04<00:20, 19.79it/s]\nGenerating text  | 88/500 [00:04<00:22, 17.99it/s]\nGenerating text  | 90/500 [00:04<00:23, 17.15it/s]\nGenerating text  | 93/500 [00:05<00:22, 18.49it/s]\nGenerating text  | 95/500 [00:05<00:21, 18.65it/s]\nGenerating text  | 97/500 [00:05<00:21, 18.86it/s]\nGenerating text  | 100/500 [00:05<00:20, 19.91it/s]\nGenerating text  | 103/500 [00:05<00:19, 20.29it/s]\nGenerating text  | 106/500 [00:05<00:19, 20.61it/s]\nGenerating text  | 109/500 [00:05<00:18, 21.12it/s]\nGenerating text  | 112/500 [00:05<00:17, 21.75it/s]\nGenerating text  | 115/500 [00:06<00:17, 21.68it/s]\nGenerating text  | 118/500 [00:06<00:17, 21.25it/s]\nGenerating text  | 121/500 [00:06<00:17, 21.29it/s]\nGenerating text  | 124/500 [00:06<00:17, 21.80it/s]\nGenerating text  | 127/500 [00:06<00:17, 21.88it/s]\nGenerating text  | 130/500 [00:06<00:16, 22.33it/s]\nGenerating text  | 133/500 [00:06<00:16, 22.93it/s]\nGenerating text  | 136/500 [00:07<00:15, 23.23it/s]\nGenerating text  | 139/500 [00:07<00:15, 23.82it/s]\nGenerating text  | 142/500 [00:07<00:15, 22.93it/s]\nGenerating text  | 145/500 [00:07<00:25, 13.70it/s]\nGenerating text  | 148/500 [00:07<00:21, 16.03it/s]\nGenerating text  | 151/500 [00:07<00:20, 16.94it/s]\nGenerating text  | 154/500 [00:08<00:19, 18.15it/s]\nGenerating text  | 157/500 [00:08<00:17, 19.12it/s]\nGenerating text  | 160/500 [00:08<00:16, 20.09it/s]\nGenerating text  | 163/500 [00:08<00:15, 21.67it/s]\nGenerating text  | 166/500 [00:08<00:15, 21.72it/s]\nGenerating text  | 169/500 [00:08<00:15, 21.48it/s]\nGenerating text  | 172/500 [00:08<00:14, 22.76it/s]\nGenerating text  | 175/500 [00:09<00:14, 23.11it/s]\nGenerating text  | 178/500 [00:09<00:13, 23.16it/s]\nGenerating text  | 181/500 [00:09<00:14, 21.51it/s]\nGenerating text  | 184/500 [00:09<00:14, 22.28it/s]\nGenerating text  | 187/500 [00:09<00:13, 23.11it/s]\nGenerating text  | 190/500 [00:09<00:13, 23.37it/s]\nGenerating text  | 193/500 [00:09<00:12, 24.33it/s]\nGenerating text  | 196/500 [00:09<00:13, 23.35it/s]\nGenerating text  | 199/500 [00:10<00:12, 23.59it/s]\nGenerating text  | 202/500 [00:10<00:12, 23.79it/s]\nGenerating text  | 205/500 [00:10<00:12, 23.55it/s]\nGenerating text  | 208/500 [00:10<00:11, 24.45it/s]\nGenerating text  | 211/500 [00:10<00:12, 23.68it/s]\nGenerating text  | 214/500 [00:10<00:11, 24.24it/s]\nGenerating text  | 217/500 [00:10<00:11, 24.24it/s]\nGenerating text  | 220/500 [00:10<00:11, 24.63it/s]\nGenerating text  | 223/500 [00:11<00:11, 23.67it/s]\nGenerating text  | 226/500 [00:11<00:12, 22.29it/s]\nGenerating text  | 229/500 [00:11<00:12, 22.15it/s]\nGenerating text  | 232/500 [00:11<00:12, 22.01it/s]\nGenerating text  | 235/500 [00:11<00:12, 21.91it/s]\nGenerating text  | 238/500 [00:11<00:11, 22.21it/s]\nGenerating text  | 241/500 [00:11<00:11, 22.06it/s]\nGenerating text  | 244/500 [00:12<00:11, 22.63it/s]\nGenerating text  | 247/500 [00:12<00:10, 23.31it/s]\nGenerating text  | 250/500 [00:12<00:10, 23.48it/s]\nGenerating text  | 253/500 [00:12<00:10, 23.40it/s]\nGenerating text  | 256/500 [00:12<00:10, 23.95it/s]\nGenerating text  | 259/500 [00:12<00:10, 23.30it/s]\nGenerating text  | 262/500 [00:12<00:10, 23.55it/s]\nGenerating text  | 265/500 [00:12<00:10, 23.33it/s]\nGenerating text  | 268/500 [00:13<00:10, 22.86it/s]\nGenerating text  | 271/500 [00:13<00:09, 23.88it/s]\nGenerating text  | 274/500 [00:13<00:09, 24.24it/s]\nGenerating text  | 277/500 [00:13<00:09, 24.15it/s]\nGenerating text  | 280/500 [00:13<00:09, 23.46it/s]\nGenerating text  | 283/500 [00:13<00:09, 23.27it/s]\nGenerating text  | 286/500 [00:13<00:09, 22.83it/s]\nGenerating text  | 289/500 [00:13<00:09, 22.73it/s]\nGenerating text  | 292/500 [00:14<00:09, 23.07it/s]\nGenerating text  | 295/500 [00:14<00:09, 22.70it/s]\nGenerating text  | 298/500 [00:14<00:09, 21.29it/s]\nGenerating text  | 301/500 [00:14<00:09, 22.00it/s]\nGenerating text  | 304/500 [00:14<00:09, 21.61it/s]\nGenerating text  | 307/500 [00:14<00:09, 21.28it/s]\nGenerating text  | 310/500 [00:14<00:09, 20.40it/s]\nGenerating text  | 313/500 [00:15<00:08, 21.30it/s]\nGenerating text  | 316/500 [00:15<00:08, 21.95it/s]\nGenerating text  | 319/500 [00:15<00:07, 22.78it/s]\nGenerating text  | 322/500 [00:15<00:07, 22.72it/s]\nGenerating text  | 325/500 [00:15<00:08, 19.86it/s]\nGenerating text  | 328/500 [00:15<00:09, 18.52it/s]\nGenerating text  | 331/500 [00:15<00:08, 19.12it/s]\nGenerating text  | 334/500 [00:16<00:08, 20.36it/s]\nGenerating text  | 337/500 [00:16<00:07, 20.60it/s]\nGenerating text  | 340/500 [00:16<00:07, 21.38it/s]\nGenerating text  | 343/500 [00:16<00:07, 22.26it/s]\nGenerating text  | 346/500 [00:16<00:06, 22.73it/s]\nGenerating text  | 349/500 [00:16<00:06, 23.10it/s]\nGenerating text  | 352/500 [00:16<00:06, 21.68it/s]\nGenerating text  | 355/500 [00:16<00:06, 22.95it/s]\nGenerating text  | 358/500 [00:17<00:06, 23.07it/s]\nGenerating text  | 361/500 [00:17<00:06, 22.34it/s]\nGenerating text  | 364/500 [00:17<00:05, 22.83it/s]\nGenerating text  | 367/500 [00:17<00:06, 21.75it/s]\nGenerating text  | 370/500 [00:17<00:05, 22.34it/s]\nGenerating text  | 373/500 [00:17<00:05, 22.63it/s]\nGenerating text  | 376/500 [00:17<00:05, 22.00it/s]\nGenerating text  | 379/500 [00:18<00:05, 21.80it/s]\nGenerating text  | 382/500 [00:18<00:05, 21.69it/s]\nGenerating text  | 385/500 [00:18<00:05, 22.01it/s]\nGenerating text  | 388/500 [00:18<00:04, 23.23it/s]\nGenerating text  | 391/500 [00:18<00:04, 24.15it/s]\nGenerating text  | 394/500 [00:18<00:04, 24.70it/s]\nGenerating text  | 397/500 [00:18<00:04, 23.89it/s]\nGenerating text  | 400/500 [00:18<00:04, 24.10it/s]\nGenerating text  | 403/500 [00:19<00:04, 23.09it/s]\nGenerating text  | 406/500 [00:19<00:04, 23.47it/s]\nGenerating text▏ | 409/500 [00:19<00:03, 24.40it/s]\nGenerating text▏ | 412/500 [00:19<00:03, 23.27it/s]\nGenerating text▎ | 415/500 [00:19<00:03, 23.51it/s]\nGenerating text▎ | 418/500 [00:19<00:03, 23.04it/s]\nGenerating text▍ | 421/500 [00:19<00:03, 23.68it/s]\nGenerating text▍ | 424/500 [00:19<00:03, 23.78it/s]\nGenerating text▌ | 427/500 [00:20<00:03, 23.23it/s]\nGenerating text▌ | 430/500 [00:20<00:03, 23.17it/s]\nGenerating text▋ | 433/500 [00:20<00:02, 22.77it/s]\nGenerating text▋ | 436/500 [00:20<00:02, 23.19it/s]\nGenerating text▊ | 439/500 [00:20<00:02, 22.50it/s]\nGenerating text▊ | 442/500 [00:20<00:02, 22.15it/s]\nGenerating text▉ | 445/500 [00:20<00:02, 22.48it/s]\nGenerating text▉ | 448/500 [00:21<00:02, 22.32it/s]\nGenerating text█ | 451/500 [00:21<00:02, 23.45it/s]\nGenerating text█ | 454/500 [00:21<00:01, 24.03it/s]\nGenerating text█▏| 457/500 [00:21<00:01, 24.79it/s]\nGenerating text█▏| 460/500 [00:21<00:01, 23.97it/s]\nGenerating text█▎| 463/500 [00:21<00:01, 23.71it/s]\nGenerating text█▎| 466/500 [00:21<00:01, 24.59it/s]\nGenerating text█▍| 469/500 [00:21<00:01, 25.25it/s]\nGenerating text█▍| 472/500 [00:22<00:01, 23.95it/s]\nGenerating text█▌| 475/500 [00:22<00:01, 22.61it/s]\nGenerating text█▌| 478/500 [00:22<00:00, 22.97it/s]\nGenerating text█▌| 481/500 [00:22<00:00, 22.96it/s]\nGenerating text█▋| 484/500 [00:22<00:00, 23.18it/s]\nGenerating text█▋| 487/500 [00:22<00:00, 23.24it/s]\nGenerating text█▊| 490/500 [00:22<00:00, 23.71it/s]\nGenerating text█▊| 493/500 [00:22<00:00, 23.10it/s]\nGenerating text█▉| 496/500 [00:23<00:00, 22.64it/s]\nGenerating text█▉| 499/500 [00:23<00:00, 23.27it/s]\n: 100%|██████████| 500/500 [00:23<00:00, 21.50it/s]","output_type":"stream"},{"name":"stdout","text":"Seed:  crouse\n    b\nGenerated text:\n,\n  but when in thee dear\n  as heart as my chase,\n  cally that beauty mish,\n  when i am not crouse\n    but when in thee dear\n  as heart as my chase,\n  cally that beauty mish,\n  when i am not crouse\n    but when in thee dear\n  as heart as my chase,\n  cally that beauty mish,\n  when i am not crouse\n    but when in thee dear\n  as heart as my chase,\n  cally that beauty mish,\n  when i am not crouse\n    but when in thee dear\n  as heart as my chase,\n  cally that beauty mish,\n  when i am not crouse\n    b\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"All we've done here is starting with a seed text, naking the input sequence, and predicting the next character on one hand. On the other hand, we shift the input sequence by removing the first character and adding the predicted character. This gives us a slightly changed sequence that still has the expected sequence length.\n\nWe then feed this updated input sequence to the model to predict another character. Repeating this process `n_chars` times will generate a text with **N** characters.","metadata":{"id":"C0RN17l36SEU"}},{"cell_type":"markdown","source":"## Conclusion","metadata":{"id":"hxx3vMH0lZOd"}},{"cell_type":"markdown","source":"It is clearly English that we are reading. However, the sentences don't make much sense. In fact, this result has several causes, in particular the length of our dataset which did not have enough samples. Also, the architecture of our model not being optimal, we find ourselves quite easily in loops repeating words ad infinitum. However, we quickly overcome this concern by adding layers to our sequential model.\n\nIn our case, after several attempts, we were able to observe **acceptable** parameters for our model. We almost have the impression that our model is really trying to **understand and write** poetry. It's funny.\n\nIt should be noted that this tutorial does not only apply to text in English but to all languages. Indeed, we could even generate code if we have enough lines of code.","metadata":{"id":"RNk7TV7RoG10"}}]}